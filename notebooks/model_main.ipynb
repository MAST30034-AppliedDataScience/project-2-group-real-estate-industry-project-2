{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from scripts.utils import create_dir, get_runtime\n",
    "import time\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 2029 data\n",
    "data_future_unit = pd.read_csv(\"../data/curated/suburb-unit-25-27.csv\")\n",
    "data_future_house = pd.read_csv(\"../data/curated/suburb-house-25-27.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_future_house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rental dataset from curated layer\n",
    "rental_df = pd.read_csv('../data/curated/rental-17-24.csv')\n",
    "rental_df['sa2_code'] = rental_df['sa2_code'].astype('Int64').astype(str)\n",
    "rental_df = rental_df[[\n",
    "    'suburb', 'sa2_code', 'type', 'year', 'bed', 'bath', 'car', 'median_income',\n",
    "    'population', 'cpi', 'unemployment_rate', 'time_city', 'avg_property_price',\n",
    "    'rented_price'\n",
    "]]\n",
    "rental_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt the graph between rented price vs year\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=rental_df, x='year', y='rented_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the graphs shows a big difference between precovid and post covid, we will only be considering data from year 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_df = rental_df[rental_df[\"year\"] >=2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year 2021 and 2022 will be used for training and year 2023 and 2024 will be used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = rental_df[\n",
    "    (rental_df['year'] >= 2021) &\n",
    "    (rental_df['year'] <= 2023)\n",
    "]\n",
    "curr_df = rental_df[\n",
    "    (rental_df['year'] > 2023) &\n",
    "    (rental_df['year'] <= 2024)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parition curr and hist dataframes based on type: 'House' and 'Unit/apmt'\n",
    "hist_house_df = hist_df[hist_df['type'] == 'House']\n",
    "hist_unit_df = hist_df[hist_df['type'] == 'Unit/apmt']\n",
    "curr_house_df = curr_df[curr_df['type'] == 'House']\n",
    "curr_unit_df = curr_df[curr_df['type'] == 'Unit/apmt']\n",
    "\n",
    "# drop 'type' column\n",
    "hist_house_df.drop(columns=['type'], inplace=True)\n",
    "hist_unit_df.drop(columns=['type'], inplace=True)\n",
    "curr_house_df.drop(columns=['type'], inplace=True)\n",
    "curr_unit_df.drop(columns=['type'], inplace=True)\n",
    "\n",
    "# print the shape of each dataframe\n",
    "print('hist_house_df:', hist_house_df.shape)\n",
    "print('hist_unit_df:', hist_unit_df.shape)\n",
    "print('curr_house_df:', curr_house_df.shape)\n",
    "print('curr_unit_df:', curr_unit_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irregular data\n",
    "\n",
    "hist_house_df = hist_house_df[hist_house_df[\"bath\"] < 5]\n",
    "curr_house_df = curr_house_df[curr_house_df[\"bath\"] < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INSTANCES_PER_SUBURB = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of suburbs with more than 1000 instances\n",
    "hist_house_df['count'] = hist_house_df.groupby('suburb')['suburb'].transform('count')\n",
    "hist_house_df[hist_house_df['count'] >= MIN_INSTANCES_PER_SUBURB]['suburb'].nunique()\n",
    "\n",
    "hist_unit_df['count'] = hist_unit_df.groupby('suburb')['suburb'].transform('count')\n",
    "\n",
    "curr_house_df['count'] = curr_house_df.groupby('suburb')['suburb'].transform('count')\n",
    "curr_unit_df['count'] = curr_unit_df.groupby('suburb')['suburb'].transform('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add a new column which is the count of the entries within that suburb\n",
    "\n",
    "high_confidence_unit_hist = hist_unit_df[hist_unit_df['count'] >= MIN_INSTANCES_PER_SUBURB]\n",
    "high_confidence_house_hist = hist_house_df[hist_house_df['count'] >= MIN_INSTANCES_PER_SUBURB]\n",
    "\n",
    "low_confidence_unit_hist = hist_unit_df[hist_unit_df['count'] < MIN_INSTANCES_PER_SUBURB]\n",
    "low_confidence_house_hist = hist_house_df[hist_house_df['count'] < MIN_INSTANCES_PER_SUBURB]\n",
    "\n",
    "high_unit_confidence_suburb = high_confidence_unit_hist['suburb'].unique()\n",
    "low_unit_confidence_suburb = low_confidence_unit_hist['suburb'].unique()\n",
    "\n",
    "high_house_confidence_suburb = high_confidence_house_hist['suburb'].unique()\n",
    "low_house_confidence_suburb = low_confidence_house_hist['suburb'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_confidence_house_curr = curr_house_df[curr_house_df['suburb'].isin(high_house_confidence_suburb)]\n",
    "low_confidence_house_curr = curr_house_df[curr_house_df['suburb'].isin(low_house_confidence_suburb)]\n",
    "\n",
    "high_confidence_house_curr = high_confidence_house_curr[high_confidence_house_curr['rented_price']>300]\n",
    "low_confidence_house_curr = low_confidence_house_curr[low_confidence_house_curr['rented_price']>300]\n",
    "\n",
    "high_confidence_unit_curr = curr_unit_df[curr_unit_df['suburb'].isin(high_unit_confidence_suburb)]\n",
    "low_confidence_unit_curr = curr_unit_df[curr_unit_df['suburb'].isin(low_unit_confidence_suburb)]\n",
    "\n",
    "high_confidence_unit_curr = high_confidence_unit_curr[high_confidence_unit_curr['rented_price']>200]\n",
    "low_confidence_unit_curr = low_confidence_unit_curr[low_confidence_unit_curr['rented_price']>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['bed, bath, car, year, median_income, population, cpi, unemployment_rate, time_city', 'avg_property_price',\"count\"]\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Defining the ColumnTransformer for one-hot encoding 'type' and standardizing numeric columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def run_0R(X_train, X_test, y_train):\n",
    "    zero_r_model = DummyRegressor(strategy='mean')\n",
    "    zero_r_model.fit(X_train, y_train)\n",
    "    # print training score\n",
    "    #print(\"ZeroR (Baseline) Model\")\n",
    "    #print(f\"Training Score: {zero_r_model.score(X_train, y_train)}\")\n",
    "    return zero_r_model.predict(X_test)\n",
    "\n",
    "def run_LR(X_train, X_test, y_train):\n",
    "    # Initialize and fit the linear regression model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the predicted values\n",
    "    predictions = lr_model.predict(X_test)\n",
    "    \n",
    "    # Extract the model coefficients\n",
    "    coef = lr_model.coef_\n",
    "    \n",
    "    # Create a DataFrame to pair feature names with their corresponding coefficients\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Coefficient': coef\n",
    "    })\n",
    "    \n",
    "    # Sort the DataFrame by the absolute value of the coefficients (most relevant feature first)\n",
    "    feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
    "    feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    # Display the most relevant features (sorted by the coefficient's absolute value)\n",
    "    \n",
    "    \n",
    "    return predictions,feature_importance\n",
    "\n",
    "def run_RF(X_train, X_test, y_train):\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=1, max_depth=5, min_samples_split=5)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    return rf_model.predict(X_test)\n",
    "\n",
    "def run_MLP(X_train, X_test, y_train):\n",
    "    mlp_model = MLPRegressor(random_state=1, max_iter=300)\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "    #print(\"ZeroR (Baseline) Model\")\n",
    "    #print(f\"Training Score: {mlp_model.score(X_train, y_train)}\")\n",
    "    return mlp_model.predict(X_test)\n",
    "\n",
    "def run_KNN(X_train, X_test, y_train):\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    #print(\"ZeroR (Baseline) Model\")\n",
    "    #print(f\"Training Score: {knn_model.score(X_train, y_train)}\")\n",
    "    return knn_model.predict(X_test)\n",
    "\n",
    "def run_SVR(X_train, X_test, y_train):\n",
    "    from sklearn.svm import SVR\n",
    "    svr_model = SVR(kernel='linear')\n",
    "    svr_model.fit(X_train, y_train)\n",
    "    #print(\"ZeroR (Baseline) Model\")\n",
    "    #print(f\"Training Score: {svr_model.score(X_train, y_train)}\")\n",
    "    return svr_model.predict(X_test)\n",
    "\n",
    "def run_combine_models(X_train, X_test, y_train, weight_lr=0.5, weight_rf=0.5):\n",
    "    # Ensure weights sum to 1\n",
    "    assert weight_lr + weight_rf == 1, \"The sum of the weights should be 1.\"\n",
    "    \n",
    "    # --- Linear Regression ---\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    lr_predictions = lr_model.predict(X_test)\n",
    "    \n",
    "    # Extract the model coefficients for LR\n",
    "    coef = lr_model.coef_\n",
    "    feature_importance_lr = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Coefficient': coef\n",
    "    })\n",
    "    feature_importance_lr['Abs_Coefficient'] = feature_importance_lr['Coefficient'].abs()\n",
    "    feature_importance_lr = feature_importance_lr.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=1, max_depth=5, min_samples_split=5)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    \n",
    "    # Combine predictions by averaging (weighted average can be applied)\n",
    "    combined_predictions = (weight_lr * lr_predictions) + (weight_rf * rf_predictions)\n",
    "\n",
    "    # You can choose to return the feature importances for both models or just LR\n",
    "    return combined_predictions, feature_importance_lr\n",
    "\n",
    "\n",
    "\n",
    "def display_metrics(y_test, y_pred_zero_r,y_pred_lr,y_pred_rf):\n",
    "    # Compute evaluation metrics for ZeroR\n",
    "    mse_zero_r = mean_squared_error(y_test, y_pred_zero_r)\n",
    "    r2_zero_r = r2_score(y_test, y_pred_zero_r)\n",
    "    # Compute evaluation metrics for Linear Regression\n",
    "    mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "    r2_lr = r2_score(y_test, y_pred_lr)\n",
    "    # Compute evaluation metrics for Random Forest\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "    # get variance of the target variable\n",
    "    variance = y_test.var()      \n",
    "    # Output the results\n",
    "    print(\"ZeroR (Baseline) Model\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse_zero_r}\")\n",
    "    print(f\"R-Squared (R2 Score): {r2_zero_r}\\n\")\n",
    "    print(\"Linear Regression Model\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse_lr}\")\n",
    "    print(f\"R-Squared (R2 Score): {r2_lr}\\n\")\n",
    "    print(\"Random Forest Model\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse_rf}\")\n",
    "    print(f\"R-Squared (R2 Score): {r2_rf}\")\n",
    "    print(f\"Variance of the target variable: {variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def model_and_predict(df1, future_df):\n",
    "    Prior_2021 = df1\n",
    "    # set intersection of suburbs in the future data and the training data\n",
    "    suburb_testing = set(future_df['suburb'].unique().tolist()).intersection(set(Prior_2021['suburb'].unique().tolist()))\n",
    "    list_future_df = []\n",
    "    missing_suburbs = []\n",
    "    print(\"Suburbs to predict: \", len(suburb_testing))\n",
    "    for i in tqdm(suburb_testing):\n",
    "        # Filter the data by suburb\n",
    "        Prior_2021_suburb = Prior_2021[Prior_2021['suburb'] == i]\n",
    "        if Prior_2021_suburb.shape[0] == 0:\n",
    "            missing_suburbs.append(i)\n",
    "            continue\n",
    "        X_train = Prior_2021_suburb.drop(columns=['rented_price', 'suburb', 'sa2_code','count'])\n",
    "        y_train = Prior_2021_suburb['rented_price']\n",
    "        \n",
    "        col_names = X_train.columns\n",
    "        \n",
    "        xy_future = future_df[future_df['suburb'] == i].copy()\n",
    "        x_future = xy_future.drop(columns=[ 'suburb','rented_price'])\n",
    "        x_future = x_future.reindex(col_names, axis=1)\n",
    "\n",
    "        # future prediction\n",
    "        if not x_future.empty:\n",
    "            xy_future.loc[:,'pred_0r'] = run_0R(X_train, x_future, y_train)\n",
    "            xy_future.loc[:,'pred_lr'],_ = run_LR(X_train, x_future, y_train)\n",
    "            xy_future.loc[:,'pred_rf'] = run_RF(X_train, x_future, y_train)\n",
    "            xy_future.loc[:,'pred_combine'],_ = run_combine_models(X_train, x_future, y_train, weight_lr=0.5, weight_rf=0.5)\n",
    "        else:\n",
    "            tqdm.write(f\"Suburb {i} has no future data\")\n",
    "            xy_future.loc[:,'pred_0r'] = np.nan\n",
    "            xy_future.loc[:,'pred_lr'] = np.nan\n",
    "            xy_future.loc[:,'pred_rf'] = np.nan\n",
    "            xy_future.loc[:,'pred_combine'] = np.nan\n",
    "            \n",
    "        list_future_df.append(xy_future)\n",
    "    return pd.concat(list_future_df), missing_suburbs\n",
    "    \n",
    "    \n",
    "def modeling(df1, df2):\n",
    "    Prior_2021 = df1\n",
    "    After_2021 = df2\n",
    "\n",
    "    suburb_testing = After_2021['suburb'].unique()\n",
    "    \n",
    "    # Initialize lists to collect results\n",
    "    average_mse_zero_r = []\n",
    "    average_mse_lr = []\n",
    "    average_mse_rf = []\n",
    "    average_r2_combine = []\n",
    "    \n",
    "    bad_prediction = []\n",
    "    good_prediction = []\n",
    "    \n",
    "    # Initialize dictionary to aggregate feature importances across all suburbs\n",
    "    feature_importance_sum = pd.DataFrame()\n",
    "    # Loop over all suburbs\n",
    "    for i in tqdm(suburb_testing):\n",
    "        # Filter the data by suburb\n",
    "        Prior_2021_suburb = Prior_2021[Prior_2021['suburb'] == i]\n",
    "        After_2021_suburb = After_2021[After_2021['suburb'] == i]\n",
    "\n",
    "        X_train = Prior_2021_suburb.drop(columns=['rented_price', 'suburb', 'sa2_code','count'])\n",
    "        X_test = After_2021_suburb.drop(columns=['rented_price', 'suburb', 'sa2_code','count'])\n",
    "        y_train = Prior_2021_suburb['rented_price']\n",
    "        y_test = After_2021_suburb['rented_price']\n",
    "        \n",
    "    \n",
    "        \n",
    "        # Run the models\n",
    "        y_pred_zero_r = run_0R(X_train, X_test, y_train)\n",
    "        y_pred_lr, feature_importance = run_LR(X_train, X_test, y_train)\n",
    "        y_pred_rf = run_RF(X_train, X_test, y_train)\n",
    "        y_pred_combine, _ = run_combine_models(X_train, X_test, y_train, weight_lr=0.5, weight_rf=0.5)\n",
    "        # future prediction\n",
    "        \n",
    "        # Calculate R2 scores\n",
    "        r2_lr = r2_score(y_test, y_pred_lr)\n",
    "        r2_rf = r2_score(y_test, y_pred_rf)\n",
    "        r2_combine = r2_score(y_test, y_pred_combine)\n",
    "\n",
    "        # Store the R2 results for analysis\n",
    "        average_mse_zero_r.append(r2_score(y_test, y_pred_zero_r))\n",
    "        average_mse_lr.append(r2_lr)\n",
    "        average_mse_rf.append(r2_rf)\n",
    "        average_r2_combine.append(r2_combine)\n",
    "        \n",
    "        # Sum up feature importances across suburbs for LR\n",
    "        if feature_importance_sum.empty:\n",
    "            feature_importance_sum = feature_importance.set_index('Feature')\n",
    "        else:\n",
    "            feature_importance_sum['Coefficient'] += feature_importance.set_index('Feature')['Coefficient']\n",
    "\n",
    "    res_df = pd.DataFrame({'suburb':After_2021['suburb'].unique()})\n",
    "    res_df['r2_0r'] = average_mse_lr\n",
    "    res_df['r2_lr'] = average_mse_lr\n",
    "    res_df['r2_rf'] = average_mse_rf\n",
    "    res_df['r2_combine'] = average_r2_combine\n",
    "    # Remove negative R2 values\n",
    "    average_mse_lr = [x for x in average_mse_lr if x >= 0]\n",
    "    average_mse_rf = [x for x in average_mse_rf if x >= 0] \n",
    "    average_r2_combine = [x for x in average_r2_combine if x >= 0]\n",
    "    \n",
    "    # Print average R2 scores for LR, RF, and Combined\n",
    "    print('Average R2 Score LR:', np.mean(average_mse_lr))\n",
    "    print('Average R2 Score RF:', np.mean(average_mse_rf))\n",
    "    print('Average R2 Score Combined:', np.mean(average_r2_combine))\n",
    "    \n",
    "    # Print bad and good predictions if applicable\n",
    "    # print(bad_prediction)\n",
    "    # print(good_prediction)\n",
    "\n",
    "    # Find the top 5 most important features overall (based on summed coefficients from LR)\n",
    "    feature_importance_sum['Abs_Coefficient'] = feature_importance_sum['Coefficient'].abs()\n",
    "    top_5_features = feature_importance_sum.sort_values(by='Abs_Coefficient', ascending=False).head(5)\n",
    "\n",
    "    # Print the top 5 most important features\n",
    "    print(\"Top 5 most important features across all suburbs:\")\n",
    "    print(top_5_features[['Coefficient', 'Abs_Coefficient']])\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling high confidnece house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Flatten the axes array for easy iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Loop over the years and plot each year's distribution in a separate subplot\n",
    "# for idx, i in enumerate(range(2017, 2025)):\n",
    "#     wyndham_vale = rental_df[(rental_df['suburb'] == 'BALLARAT CENTRAL') & (rental_df['year'] == i)]\n",
    "#     sns.histplot(wyndham_vale[\"rented_price\"], kde=True, bins=30, ax=axes[idx])\n",
    "#     axes[idx].set_title(f'MELBOURNE Rented Price Distribution in {i}')\n",
    "#     axes[idx].set_xlabel('Rented Price')\n",
    "#     axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the combined plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loop over the years and plot each year's distribution on the same figure\n",
    "for i in range(2017, 2025):\n",
    "    wyndham_vale = rental_df[(rental_df['suburb'] == 'SOUTH YARRA') & (rental_df['year'] == i)]\n",
    "    sns.histplot(wyndham_vale[\"rented_price\"], kde=True, label=f'{i}', bins=30)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('MELBOURNE Rented Price Distribution (2021-2024)')\n",
    "plt.xlabel('Rented Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Year')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "low_confidence_house_hist#[low_confidence_house_curr['suburb'] == 'Fitzroy (Vic.)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# sorted(low_confidence_house_hist['suburb'].unique().tolist())[250:]\n",
    "\n",
    "low_confidence_house_hist[low_confidence_house_hist['suburb'] == 'Fitzroy (Vic.)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = high_confidence_house_hist\n",
    "df2 = high_confidence_house_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score_df = modeling(df1,df2)\n",
    "r2_score_df['type'] = 'House'\n",
    "r2_score_df['confidence'] = 'high'\n",
    "r2_list.append(r2_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling high confidence unit/apmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bathroom should be less than 2\n",
    "\n",
    "high_confidence_unit_hist = high_confidence_unit_hist[high_confidence_unit_hist['bath'] <= 2]\n",
    "high_confidence_unit_curr = high_confidence_unit_curr[high_confidence_unit_curr['bath'] <= 2]\n",
    "\n",
    "high_confidence_unit_hist = high_confidence_unit_hist[high_confidence_unit_hist['bath'] > 0]\n",
    "high_confidence_unit_curr = high_confidence_unit_curr[high_confidence_unit_curr['bath'] > 0]\n",
    "\n",
    "# car park should be less than 3\n",
    "\n",
    "high_confidence_unit_hist = high_confidence_unit_hist[high_confidence_unit_hist['car'] <= 3]\n",
    "high_confidence_unit_curr = high_confidence_unit_curr[high_confidence_unit_curr['car'] <= 3]\n",
    "\n",
    "# eliminate apartment with more than 1400 per week\n",
    "\n",
    "high_confidence_unit_hist = high_confidence_unit_hist[high_confidence_unit_hist['rented_price'] <= 1400]\n",
    "high_confidence_unit_curr = high_confidence_unit_curr[high_confidence_unit_curr['rented_price'] <= 1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = high_confidence_unit_hist\n",
    "df2 = high_confidence_unit_curr\n",
    "\n",
    "r2_score_df = modeling(df1,df2)\n",
    "r2_score_df['type'] = 'Unit'\n",
    "r2_score_df['confidence'] = 'high'\n",
    "r2_list.append(r2_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # taking Melbourne city as an example to observe the model performance\n",
    "\n",
    "# Melbourne_train = high_confidence_unit_hist[high_confidence_unit_hist['suburb'] == 'MELBOURNE']\n",
    "# Melbourne_test = high_confidence_unit_curr[high_confidence_unit_curr['suburb'] == 'MELBOURNE']\n",
    "\n",
    "# X_train = Melbourne_train.drop(columns=['rented_price','suburb','sa2_code'])\n",
    "# X_test = Melbourne_test.drop(columns=['rented_price','suburb','sa2_code'])\n",
    "# y_train = Melbourne_train['rented_price']\n",
    "# y_test = Melbourne_test['rented_price']\n",
    "\n",
    "\n",
    "# def run_RF(X_train, X_test, y_train):\n",
    "#     rf_model = RandomForestRegressor(n_estimators=150, random_state=1, max_depth=5)\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "#     #print(\"ZeroR (Baseline) Model\")\n",
    "#     # print important features\n",
    "#     importances = rf_model.feature_importances_\n",
    "#     feature_names = X_train.columns  # If X_train is a pandas DataFrame\n",
    "#     feature_importance_df = pd.DataFrame({\n",
    "#         'Feature': feature_names,\n",
    "#         'Importance': importances\n",
    "#     })\n",
    "#     feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "#     # print(feature_importance_df.head())  # Display the top features\n",
    "\n",
    "#     return rf_model.predict(X_test)\n",
    "\n",
    "# def run_logistic(X_train, X_test, y_train):\n",
    "#     from sklearn.linear_model import LogisticRegression\n",
    "#     logistic_model = LogisticRegression(random_state=1)\n",
    "#     logistic_model.fit(X_train, y_train)\n",
    "#     #print(\"ZeroR (Baseline) Model\")\n",
    "#     #print(f\"Training Score: {logistic_model.score(X_train, y_train)}\")\n",
    "#     return logistic_model.predict(X_test)\n",
    "\n",
    "# def run_ridge(X_train, X_test, y_train):\n",
    "#     from sklearn.linear_model import Ridge\n",
    "#     ridge_model = Ridge(alpha=1.0)\n",
    "#     ridge_model.fit(X_train, y_train)\n",
    "#     #print(\"ZeroR (Baseline) Model\")\n",
    "#     #print(f\"Training Score: {ridge_model.score(X_train, y_train)}\")\n",
    "#     return ridge_model.predict(X_test)\n",
    "\n",
    "# y_pred_rf = run_RF(X_train, X_test, y_train)\n",
    "# y_pred_ridge = run_ridge(X_train, X_test, y_train)\n",
    "\n",
    "\n",
    "# r2_rf = r2_score(y_test, y_pred_rf)\n",
    "# r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "\n",
    "# print(r2_rf,r2_ridge)\n",
    "# Melbourne_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling low confidence house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = low_confidence_unit_hist\n",
    "df2 = low_confidence_unit_curr\n",
    "r2_score_df = modeling(df1,df2)\n",
    "r2_score_df['type'] = 'House'\n",
    "r2_score_df['confidence'] = 'low'\n",
    "r2_list.append(r2_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling low confidence unit/apmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = low_confidence_unit_hist\n",
    "df2 = low_confidence_unit_curr\n",
    "r2_score_df = modeling(df1,df2)\n",
    "r2_score_df['type'] = 'Unit'\n",
    "r2_score_df['confidence'] = 'low'\n",
    "r2_list.append(r2_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a correlation map between all variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(high_confidence_house_hist.drop(columns=[\"suburb\"]).corr(), annot=True, cmap='coolwarm', center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2 score out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(r2_list).to_csv(\"../data/curated/r2_score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_house_high, missing_house_high = model_and_predict(high_confidence_house_hist,data_future_house)\n",
    "predict_house_high['confidence'] = 'high'\n",
    "\n",
    "predict_unit_high, missing_unit_high = model_and_predict(high_confidence_unit_hist, data_future_unit)\n",
    "predict_unit_high['confidence'] = 'high'\n",
    "\n",
    "predict_house_low, missing_house_low = model_and_predict(low_confidence_house_hist,data_future_house)\n",
    "predict_house_low['confidence'] = 'low'\n",
    "\n",
    "predict_unit_low, missing_unit_low = model_and_predict(low_confidence_unit_hist,data_future_unit)\n",
    "predict_unit_low['confidence'] = 'low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_sub = predict_house_high['suburb'].unique().tolist()\n",
    "predict_house_low = predict_house_low[~predict_house_low['suburb'].isin(house_sub)]\n",
    "predict_house = pd.concat([predict_house_high, predict_house_low])\n",
    "predict_house.sort_index(inplace=True)\n",
    "predict_house.to_csv(\"../data/curated/predict_house.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_sub  = predict_unit_high['suburb'].unique().tolist()\n",
    "predict_unit_low = predict_unit_low[~predict_unit_low['suburb'].isin(unit_sub)]\n",
    "predict_unit = pd.concat([predict_unit_high, predict_unit_low])\n",
    "predict_unit.sort_index(inplace=True)\n",
    "predict_unit.to_csv(\"../data/curated/predict_unit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_house['suburb'].nunique(), predict_unit['suburb'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count how many suburbs processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_l = sorted(hist_unit_df['suburb'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_suburb = []\n",
    "for subrub in data_future_house['suburb'].unique():\n",
    "    if subrub not in predict_house['suburb'].unique():\n",
    "        missing_suburb.append(subrub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_suburb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_both = []\n",
    "not_both = []\n",
    "for subrub in missing_suburb:\n",
    "    if subrub in suburb_l:\n",
    "        in_both.append(subrub)\n",
    "    else:\n",
    "        not_both.append(subrub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = hist_house_df[hist_house_df['suburb'].isin(in_both)]\n",
    "temp.groupby('suburb')['suburb'].count().sort_index()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
